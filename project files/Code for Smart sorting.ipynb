{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPzZRTkwFW43EXZu3JsQh1R"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["# ==============================\n","# âœ… Install Dependencies\n","# ==============================\n","!pip install -q gradio tensorflow matplotlib seaborn opencv-python\n","\n","# ==============================\n","# ğŸ“¦ Import Libraries\n","# ==============================\n","import os\n","import numpy as np\n","import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras import layers\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from tensorflow.keras.applications import EfficientNetB0\n","from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n","import matplotlib.pyplot as plt\n","import cv2\n","import random\n","from PIL import Image\n","from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n","import gradio as gr\n","\n","# ==============================\n","# ğŸ§ª Set Seeds & Constants\n","# ==============================\n","np.random.seed(42)\n","tf.random.set_seed(42)\n","\n","IMG_SIZE = (224, 224)\n","BATCH_SIZE = 32\n","EPOCHS = 10\n","NUM_CLASSES = 2\n","LEARNING_RATE = 0.0001\n","\n","# ==============================\n","# ğŸ“ Generate Synthetic Dataset\n","# ==============================\n","def create_sample_dataset():\n","    for category in ['train', 'validation', 'test']:\n","        for label in ['fresh', 'rotten']:\n","            os.makedirs(f'dataset/{category}/{label}', exist_ok=True)\n","\n","    def create_sample_image(path, color, text):\n","        img = np.zeros((224, 224, 3), dtype=np.uint8)\n","        img[:, :, :] = color\n","        cv2.putText(img, text, (50, 120), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 3)\n","        cv2.imwrite(path, img)\n","\n","    for i in range(50):\n","        create_sample_image(f'dataset/train/fresh/fresh_{i}.jpg', [0, 150, 0], \"FRESH\")\n","        create_sample_image(f'dataset/train/rotten/rotten_{i}.jpg', [150, 0, 0], \"ROTTEN\")\n","\n","    for i in range(15):\n","        create_sample_image(f'dataset/validation/fresh/fresh_{i}.jpg', [0, 150, 0], \"FRESH\")\n","        create_sample_image(f'dataset/validation/rotten/rotten_{i}.jpg', [150, 0, 0], \"ROTTEN\")\n","        create_sample_image(f'dataset/test/fresh/fresh_{i}.jpg', [0, 150, 0], \"FRESH\")\n","        create_sample_image(f'dataset/test/rotten/rotten_{i}.jpg', [150, 0, 0], \"ROTTEN\")\n","\n","create_sample_dataset()\n","\n","# ==============================\n","# ğŸš€ Data Loaders\n","# ==============================\n","train_datagen = ImageDataGenerator(rescale=1./255, rotation_range=20, width_shift_range=0.1,\n","                                   height_shift_range=0.1, shear_range=0.1, zoom_range=0.1,\n","                                   horizontal_flip=True, fill_mode='nearest')\n","val_test_datagen = ImageDataGenerator(rescale=1./255)\n","\n","train_generator = train_datagen.flow_from_directory('dataset/train', target_size=IMG_SIZE,\n","                                                    batch_size=BATCH_SIZE, class_mode='categorical')\n","val_generator = val_test_datagen.flow_from_directory('dataset/validation', target_size=IMG_SIZE,\n","                                                     batch_size=BATCH_SIZE, class_mode='categorical')\n","test_generator = val_test_datagen.flow_from_directory('dataset/test', target_size=IMG_SIZE,\n","                                                      batch_size=BATCH_SIZE, class_mode='categorical', shuffle=False)\n","\n","# ==============================\n","# ğŸ§  Build Model\n","# ==============================\n","base_model = EfficientNetB0(input_shape=(224, 224, 3), include_top=False, weights='imagenet')\n","base_model.trainable = False\n","\n","inputs = keras.Input(shape=(224, 224, 3))\n","x = base_model(inputs, training=False)\n","x = layers.GlobalAveragePooling2D()(x)\n","x = layers.Dense(128, activation='relu')(x)\n","x = layers.Dropout(0.3)(x)\n","outputs = layers.Dense(NUM_CLASSES, activation='softmax')(x)\n","\n","model = keras.Model(inputs, outputs)\n","model.compile(optimizer=keras.optimizers.Adam(learning_rate=LEARNING_RATE),\n","              loss='categorical_crossentropy', metrics=['accuracy'])\n","\n","# ==============================\n","# ğŸ‹ Train Model\n","# ==============================\n","callbacks = [EarlyStopping(patience=3, restore_best_weights=True),\n","             ModelCheckpoint(\"best_model.h5\", save_best_only=True, monitor='val_accuracy')]\n","\n","history = model.fit(train_generator, steps_per_epoch=train_generator.samples // BATCH_SIZE,\n","                    validation_data=val_generator,\n","                    validation_steps=val_generator.samples // BATCH_SIZE,\n","                    epochs=EPOCHS, callbacks=callbacks)\n","\n","# ==============================\n","# ğŸ“Š Confusion Matrix\n","# ==============================\n","y_true = test_generator.classes\n","y_pred_probs = model.predict(test_generator)\n","y_pred = np.argmax(y_pred_probs, axis=1)\n","class_labels = list(train_generator.class_indices.keys())\n","\n","cm = confusion_matrix(y_true, y_pred)\n","ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_labels).plot(cmap='Blues', values_format='d')\n","plt.title(\"Confusion Matrix\")\n","plt.savefig(\"confusion_matrix.png\")\n","plt.close()\n","\n","# ==============================\n","# ğŸ¯ Use Fixed Test Samples\n","# ==============================\n","fixed_test_images = [\n","    'dataset/test/fresh/fresh_0.jpg',\n","    'dataset/test/fresh/fresh_1.jpg',\n","    'dataset/test/rotten/rotten_0.jpg',\n","    'dataset/test/rotten/rotten_1.jpg',\n","]\n","\n","def load_and_predict_fixed_images(image_paths):\n","    results = []\n","    for path in image_paths:\n","        img = Image.open(path).resize(IMG_SIZE)\n","        img_array = np.expand_dims(np.array(img) / 255.0, axis=0)\n","        prediction = model.predict(img_array)\n","        class_idx = np.argmax(prediction[0])\n","        confidence = float(np.max(prediction[0]))\n","        predicted_label = class_labels[class_idx]\n","\n","        annotated = np.array(img).copy()\n","        annotated = cv2.putText(annotated, f\"{predicted_label.upper()} ({confidence:.2f})\", (10, 30),\n","                                cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)\n","        results.append(Image.fromarray(annotated))\n","    return results\n","\n","sample_images = load_and_predict_fixed_images(fixed_test_images)\n","\n","# ==============================\n","# ğŸ–¼ Gradio UI (No Upload)\n","# ==============================\n","with gr.Blocks() as demo:\n","    gr.Markdown(\"## ğŸ§  Smart Fruit Freshness Classifier\")\n","\n","    gr.Markdown(\"### ğŸ“Š Confusion Matrix\")\n","    gr.Image(\"confusion_matrix.png\")\n","\n","    gr.Markdown(\"### ğŸ¯ Predictions on Fixed Test Samples\")\n","    with gr.Row():\n","        for sample_img in sample_images:\n","            gr.Image(value=sample_img, show_label=False)\n","\n","demo.launch()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"Qm2fu-Qm_v6c","executionInfo":{"status":"ok","timestamp":1751139735446,"user_tz":-330,"elapsed":81941,"user":{"displayName":"poojitha sri anjali sama | AP23110020114","userId":"06291439492010826674"}},"outputId":"e6df2266-fabb-4bfd-e010-9f3607e2a5e9","collapsed":true},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Found 100 images belonging to 2 classes.\n","Found 30 images belonging to 2 classes.\n","Found 30 images belonging to 2 classes.\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n","  self._warn_if_super_not_called()\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n","\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.4023 - loss: 0.7478   "]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n","  self._warn_if_super_not_called()\n","WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 5s/step - accuracy: 0.4047 - loss: 0.7447 - val_accuracy: 0.5000 - val_loss: 0.6970\n","Epoch 2/10\n","\u001b[1m1/3\u001b[0m \u001b[32mâ”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[1m7s\u001b[0m 4s/step - accuracy: 0.5312 - loss: 0.7056"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/keras/src/trainers/epoch_iterator.py:107: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n","  self._interrupted_warning()\n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 931ms/step - accuracy: 0.5312 - loss: 0.7056 - val_accuracy: 0.5000 - val_loss: 0.6991\n","Epoch 3/10\n","\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 5s/step - accuracy: 0.5039 - loss: 0.7098 - val_accuracy: 0.5000 - val_loss: 0.7002\n","Epoch 4/10\n","\u001b[1m1/3\u001b[0m \u001b[32mâ”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[1m0s\u001b[0m 199ms/step - accuracy: 0.7500 - loss: 0.6307"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/keras/src/trainers/epoch_iterator.py:107: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n","  self._interrupted_warning()\n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1s/step - accuracy: 0.7500 - loss: 0.6307 - val_accuracy: 0.5000 - val_loss: 0.6997\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n","  self._warn_if_super_not_called()\n","WARNING:tensorflow:5 out of the last 11 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x7ec3cc457a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"]},{"output_type":"stream","name":"stdout","text":["\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4s/step\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:5 out of the last 11 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x7ec3cc457a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"]},{"output_type":"stream","name":"stdout","text":["\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step\n","\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 98ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 98ms/step\n","\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step\n","It looks like you are running Gradio on a hosted a Jupyter notebook. For the Gradio app to work, sharing must be enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n","\n","Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n","* Running on public URL: https://6bfa4d68cf0fd5da1b.gradio.live\n","\n","This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<div><iframe src=\"https://6bfa4d68cf0fd5da1b.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"]},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":[]},"metadata":{},"execution_count":3}]}]}